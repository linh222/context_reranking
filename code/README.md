# Lifelog Context Retrieval - CBMI 2025
## Implementation Code

This folder contains the complete implementation code for the lifelog context retrieval system presented in the CBMI 2025 paper. The code is generated by Claude Sonnet 4 Agent from the original code implemented by the authors.

## üìÅ File Structure

```
paper/code/
‚îú‚îÄ‚îÄ lifelog_retrieval.py     # Core retrieval system implementation
‚îú‚îÄ‚îÄ train_models.py          # Training scripts for all models
‚îú‚îÄ‚îÄ evaluate_models.py       # Comprehensive evaluation framework
‚îú‚îÄ‚îÄ demo.py                  # Interactive demonstration script
‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies
‚îî‚îÄ‚îÄ README.md               # This file
```

## üöÄ Quick Start

### Installation

1. Create a virtual environment:
```bash
python -m venv lifelog_env
source lifelog_env/bin/activate  # On Windows: lifelog_env\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

### Basic Usage

Run the interactive demonstration:
```bash
python demo.py
```

For component-specific demonstrations:
```bash
python demo.py --component text        # Text retrieval only
python demo.py --component multimodal  # Multimodal retrieval
python demo.py --component training    # Training pipeline
python demo.py --component evaluation  # Evaluation metrics
```

## üîß Core Components

### 1. Lifelog Retrieval System (`lifelog_retrieval.py`)

The main system consists of several key components:

- **TextualRetriever**: BERT-based text encoder for semantic similarity
- **VisualEncoder**: CLIP-based visual feature extraction
- **RerankerModel**: Neural reranking model for improved precision
- **MultimodalRetriever**: Combined text-visual retrieval system

#### Example Usage:

```python
from lifelog_retrieval import TextualRetriever, VisualEncoder, MultimodalRetriever

# Initialize components
text_retriever = TextualRetriever("bert-base-uncased")
visual_encoder = VisualEncoder("clip")
multimodal_retriever = MultimodalRetriever(text_retriever, visual_encoder)

# Retrieve contexts for a query
query = "What did I eat for lunch?"
contexts = ["Had sandwich at cafe", "Meeting with colleagues", "Walk in park"]
results = multimodal_retriever.retrieve_and_rerank(query, contexts, [], top_k=3)
```

### 2. Training Pipeline (`train_models.py`)

Comprehensive training framework supporting:

- **Reranker Training**: Fine-tune BERT for query-context relevance
- **Retriever Fine-tuning**: Adapt pre-trained models for lifelog domain
- **Multi-GPU Training**: Distributed training support
- **Hyperparameter Optimization**: Grid search and random search

#### Training a Reranker:

```bash
python train_models.py \
    --data_path /path/to/training_data.csv \
    --train_reranker \
    --num_epochs 5 \
    --learning_rate 2e-5 \
    --save_dir models/
```

### 3. Evaluation Framework (`evaluate_models.py`)

Comprehensive evaluation with standard IR metrics:

- **MRR (Mean Reciprocal Rank)**
- **MAP (Mean Average Precision)** 
- **Recall@K and Precision@K**
- **NDCG@K (Normalized Discounted Cumulative Gain)**

#### Running Evaluation:

```bash
python evaluate_models.py \
    --test_data /path/to/test_data.csv \
    --model_dir models/ \
    --results_dir results/
```

## üìä Data Format

The system expects CSV files with the following columns:

### Training Data Format:
```csv
question,context,label,ImageID
"What did I eat for breakfast?","Had eggs and toast at 8 AM",1,"img_001.jpg"
"What did I eat for breakfast?","Attended team meeting at 10 AM",0,"img_002.jpg"
```

### Test Data Format:
```csv
question,context,ImageID
"Where did I go for lunch?","Visited Italian restaurant downtown","img_lunch_001.jpg"
```

## üéØ Key Features

### Multi-Modal Retrieval
- Combines textual semantic similarity with visual information
- CLIP-based image-text alignment
- Weighted fusion of different modalities

### Neural Reranking  
- Fine-tuned BERT model for improved relevance scoring
- Supports different pre-trained models (BERT, DistilBERT, RoBERTa)
- Contrastive learning with hard negative sampling

### Scalable Architecture
- Efficient batch processing
- GPU acceleration support
- Memory-optimized for large datasets

### Comprehensive Evaluation
- Standard information retrieval metrics
- Statistical significance testing
- Visualization of results


## üî¨ Experimental Setup

### Baseline Methods
- **BM25**: Traditional keyword-based retrieval
- **BERT-Text**: BERT embeddings with cosine similarity
- **CLIP-Multimodal**: CLIP text-image matching

### Proposed Methods
- **BERT + Visual**: Combined BERT text + CLIP visual features
- **Neural Reranker**: BERT-based reranking model
- **Full Pipeline**: Complete multimodal system with reranking

## üìö Citation

If you use this code in your research, please cite our paper later.

## ü§ù Contributing

We welcome contributions! Please see the main repository for contribution guidelines.


## üìÑ License

This code is released under the MIT License. See the main repository for details.

---

**Note**: This implementation is designed for research purposes and demonstrates the methods described in our CBMI 2025 paper. For production use, additional optimizations and error handling may be required.